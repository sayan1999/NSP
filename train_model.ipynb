{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import re, os, math, random, datetime\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import adam\n",
    "from keras.losses import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 20\n",
    "embedding_vector_len=1000\n",
    "dataset_dir='dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir) if file.endswith('.txt')]\n",
    "raw_text = \" \".join([open(file).read() for file in files])\n",
    "no_of_files = len(os.listdir(dataset_dir))\n",
    "print(\"Read {} textfiles.\" .format(no_of_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing words and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [word_tokenize(sent) for sent in sent_tokenize(raw_text)]\n",
    "print(\"Total {} number of sentences.\" .format(len(sentences)))\n",
    "all_words = []\n",
    "for sent in sentences:\n",
    "    all_words += sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences, min_count=1, size=embedding_vector_len)\n",
    "new_vocab = word2vec.wv.vocab\n",
    "print(\"Vocab size {}.\" .format(len(new_vocab)))\n",
    "vocab_len=len(new_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_seqs = list(ngrams(all_words, sequence_length))\n",
    "print(\"Total number of sequences: {} with each sequence lengthing: {}.\" .format(len(all_input_seqs), sequence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_output_words = all_words[sequence_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zipping inputs and targets for consistent shuffling\n",
    "all_merged = list(zip(all_input_seqs, all_output_words))\n",
    "random.shuffle(all_merged)\n",
    "del(all_input_seqs)\n",
    "del(all_output_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break inputs into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_pass = no_of_files\n",
    "seq_len_per_pass = len(all_merged) // iteration_pass\n",
    "merged_sections = [ all_merged[i:i + seq_len_per_pass] for i in range(0, seq_len_per_pass * iteration_pass, seq_len_per_pass) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = adam(lr=learning_rate)\n",
    "loss_function = mean_absolute_error\n",
    "\n",
    "num_memory_units_1 = embedding_vector_len // 2\n",
    "num_memory_units_2 = embedding_vector_len // 4\n",
    "\n",
    "num_iterations = 30\n",
    "batch_size = 128\n",
    "words_to_generate = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name=\"NSP\")\n",
    "\n",
    "model.add(LSTM(num_memory_units_1, name='1st_LSTM_layer', return_sequences=True, input_shape=(sequence_length, embedding_vector_len)))\n",
    "model.add(LSTM(num_memory_units_2, name='2nd_LSTM_layer'))\n",
    "\n",
    "model.add(Dropout(0.2, name='1st_Dropout_layer'))\n",
    "model.add(Dense(embedding_vector_len, name='1st_Dense_layer'))\n",
    "model.add(Dropout(0.2, name='2nd_Dropout_layer'))\n",
    "model.add(Dense(embedding_vector_len, name='2nd_Dense_layer'))\n",
    "model.add(Activation('softmax', name='Activation_layer'))\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=datetime.datetime.now().strftime('saved_models/%d-%m-%y %H:%M:%S')+\".hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, verbose=1, save_frequency=5)\n",
    "# callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = 1\n",
    "for merged_section in merged_sections:\n",
    "    \n",
    "    # unzip input and target\n",
    "    input_seqs, output_words = zip(*merged_section)\n",
    "    \n",
    "    x = np.stack([word2vec[seq] for seq in input_seqs], axis=0)\n",
    "    y = word2vec[output_words]\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=22)\n",
    "\n",
    "    print(str(\"\\n\"+\"*\"*20+\"\\nRunning Secion: {}\\n\"+\"*\"*20) .format(section))\n",
    "    \n",
    "    # train the model, output generated text after each iteration\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=num_iterations, validation_data=(x_test, y_test))\n",
    "          \n",
    "    print(str(\"\\n\"+\"*\"*20+\"\\nRunning Secion: {}\\n\"+\"*\"*20) .format(section))\n",
    "    \n",
    "    section = section + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/final.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
